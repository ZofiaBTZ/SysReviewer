import urllib2
from bs4 import BeautifulSoup
import requests
from urllib2 import Request, urlopen
from pyPdf import PdfFileWriter, PdfFileReader
from StringIO import StringIO
import datetime


# create list of dois to add entries into Zotero
# e. prepare bibtex file
def download_file(url, output):
    f = open(output, 'wb')
    webFile = urllib2.urlopen(url)
    f.write(webFile.read())
    webFile.close()
    f.close()


def process_page(my_url, output_path):
    r = requests.get(my_url)
    soup_all = BeautifulSoup(r.content, "html.parser")
    titles = soup_all.find_all("h4", {"class": "paper-list-title"})
    print("titles")
    #print(titles[2])#('href')
    #print("Titles")
    for title in titles:
        new_t = title.find("a").get("href")
        print(new_t)
        url_paper = pap + new_t 
        html_paper = urllib2.urlopen(url_paper).read()
        print("next paper")
        print(url_paper)
        soup = BeautifulSoup(html_paper, 'html.parser')
        pdf_url = soup.find("meta", {"name":"citation_pdf_url"})['content']
        #title = soup.find("meta", {"name":"citation_title"})['content']
        file_name = new_t.split('/')[-1]
        output = output_path + "/" + '_'.join(file_name.split()) + '.pdf'
        download_file(pdf_url, output)

    
pap = "http://paperity.org"    
url = "http://paperity.org/search/?q="
url_n = "http://paperity.org/search/"
url_end = "?q="
query =  sys.argv[1] # "legionella"
q = url + query

def parse(){
today = datetime.date.today()
html = urllib2.urlopen(q).read()
#print(html)
print(q)
output_dir = "../" + output + str(today) + query
process_page(q, output_dir)
i = 1
while True:
    i = i + 1
    q_next = url_n + str(i) + url_end + query
    print(q_next)
    try:
        process_page(q_next, "../output_legionella")
    except Exception:
        exit
}
